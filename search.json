[{"title":"Centos 7 部署 ElasticSearch 6.7.1 集群","url":"/2021/12/10/Centos-7-%E9%83%A8%E7%BD%B2-ElasticSearch-6-7-1-%E9%9B%86%E7%BE%A4/","content":"概述实现在三台 Linux 系统上各部署一台 ElasticSearch，完成三节点的集群部署。\n下载ElasticSearch 6.7.1 官网下载地址：Elasticsearch 6.7.1 | Elastic\n步骤一、解压tar -zxvf elasticsearch-6.7.1.tar.gz\n\n二、修改配置文件（我安装的路径是：/usr/local/es）vim /usr/local/es/config/elasticsearch.yml\n\n节点1\n# 是否支持跨域http.cors.enabled: true# * 表示支持所有域名http.cors.allow-origin: &quot;*&quot;# 设置集群的名称（每个节点必须都一样）cluster.name: my-es-cluster# 设置节点名称（每个节点必须不一样）node.name: node-1# 告诉此节点，它就是 masternode.master: true# 绑定的ip地址和默认端口号9200network.host: 123.123.123# 主节点数/ 2 + 1。此值是为了阻止没有节点做主节点或者都做子节点的情况发生。避免出现脑裂。# 默认值是3，主节点数至少是4的，不然无法正常选举# 3台ES服务器，配置最少需要两台master，整个集群才可正常运行discovery.zen.minimum_master_nodes: 2# 默认端口号http.port: 9200# 集群节点的ipdiscovery.zen.ping.unicast.hosts: [&quot;123.123.123&quot;,&quot;123.123.124&quot;,&quot;123.123.125&quot;]\n\n节点2\n# 是否支持跨域http.cors.enabled: true# * 表示支持所有域名http.cors.allow-origin: &quot;*&quot;# 设置集群的名称（每个节点必须都一样）cluster.name: my-es-cluster# 设置节点名称（每个节点必须不一样）node.name: node-2# 告诉此节点，它就是 masternode.master: true# 绑定的ip地址和默认端口号9200network.host: 123.123.124# 主节点数/ 2 + 1。此值是为了阻止没有节点做主节点或者都做子节点的情况发生。避免出现脑裂。# 默认值是3，主节点数至少是4的，不然无法正常选举# 3台ES服务器，配置最少需要两台master，整个集群才可正常运行discovery.zen.minimum_master_nodes: 2# 默认端口号http.port: 9200# 集群节点的ipdiscovery.zen.ping.unicast.hosts: [&quot;123.123.123&quot;,&quot;123.123.124&quot;,&quot;123.123.125&quot;]\n\n 节点3\n# 是否支持跨域http.cors.enabled: true# * 表示支持所有域名http.cors.allow-origin: &quot;*&quot;# 设置集群的名称（每个节点必须都一样）cluster.name: my-es-cluster# 设置节点名称（每个节点必须不一样）node.name: node-3# 告诉此节点，它就是 masternode.master: true# 绑定的ip地址和默认端口号9200network.host: 123.123.125# 主节点数/ 2 + 1。此值是为了阻止没有节点做主节点或者都做子节点的情况发生。避免出现脑裂。# 默认值是3，主节点数至少是4的，不然无法正常选举# 3台ES服务器，配置最少需要两台master，整个集群才可正常运行discovery.zen.minimum_master_nodes: 2# 默认端口号http.port: 9200# 集群节点的ipdiscovery.zen.ping.unicast.hosts: [&quot;123.123.123&quot;,&quot;123.123.124&quot;,&quot;123.123.125&quot;]\n\n三、启动在启动之前需要先修改配置一些信息，否则会启动失败\n1、es 不能用 root 用户启动，需要创建一个新用户。依次执行以下命令\nuseradd es # 新建 es 用户passwd es # 设置 es 用户的密码 chown -R es:es /usr/local/es # 文件夹所有者\n\n2、 修改 /etc/security/limits.conf 分发文件\n\n命令：vim /etc/security/limits.conf\n\n在该文件末尾添加以下内容\nes soft nofile 65536es hard nofile 65536\n\n 3、修改 /etc/security/limits.d/20-nproc.conf 分发文件\n\n命令：vim /etc/security/limits.d/20-nproc.conf\n\n在文件末尾添加以下内容\nes soft nofile 65536es hard nofile 65536* hard nproc 4096\n\n 4、修改 /etc/sysctl.conf 文件\n\n命令：vim /etc/sysctl.conf\n\n在文件末尾添加以下内容\nvm.max_map_count=655360\n\n 5、重新加载\nsysctl -p\n\n根据上面步骤配置完后就可以启动了\n3台 Linux 系统的 ElasticSearch 都启动成功后，在浏览器打开以下网址：\nhttp://123.123.123:9200/_cat/nodes?v\n\n出现以下信息说明集群部署成功\n","categories":["中间件","ElasticSearch"],"tags":["ElasticSearch","集群"]},{"title":"ElasticSearch 基本命令","url":"/2022/01/12/ElasticSearch%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/","content":"查询from、size、_source\nfrom：从第几个结果开始\nsize：查询出多少个结果\n_source：列举需要显示的字段\n\nGET bank/_search&#123;  &quot;query&quot;: &#123;    &quot;match&quot;: &#123;      &quot;address&quot;: &quot;Court&quot;    &#125;  &#125;,  &quot;sort&quot;: [    &#123;      &quot;account_number&quot;: &#123;        &quot;order&quot;: &quot;desc&quot;      &#125;    &#125;  ],  &quot;from&quot;: 0,  &quot;size&quot;: 20,  &quot;_source&quot;: [&quot;account_number&quot;, &quot;firstname&quot;, &quot;lastname&quot;, &quot;address&quot;]&#125;\n\nmulti_match# 字段 address 和 lastname 中有一个包含 &quot;Celeste&quot; 或 &quot;Court&quot; 都满足要求GET bank/_search&#123;  &quot;query&quot;: &#123;    &quot;multi_match&quot;: &#123;      &quot;query&quot;: &quot;Celeste Court&quot;,      &quot;fields&quot;: [&quot;address&quot;, &quot;lastname&quot;]    &#125;  &#125;&#125;\n\nmust、not_must、should、filter\nmust：必须满足某种条件\nnot_must：必须不满足某种条件\nshould：无论满不满足都能查出来，只不过影响得分\nfilter：对结果进行过滤，没有得分\n\nGET bank/_search&#123;  &quot;query&quot;: &#123;    &quot;bool&quot;: &#123;      &quot;must&quot;: [        &#123;          &quot;match&quot;: &#123;            &quot;gender&quot;: &quot;M&quot;          &#125;        &#125;,        &#123;          &quot;match&quot;: &#123;            &quot;address&quot;: &quot;mill&quot;          &#125;        &#125;      ],      &quot;must_not&quot;: [        &#123;          &quot;match&quot;: &#123;            &quot;age&quot;: &quot;38&quot;          &#125;        &#125;      ],      &quot;should&quot;: [        &#123;          &quot;match&quot;: &#123;            &quot;balance&quot;: &quot;19648&quot;          &#125;        &#125;      ],      &quot;filter&quot;: &#123;        &quot;range&quot;: &#123;          &quot;age&quot;: &#123;            &quot;gte&quot;: 18,            &quot;lte&quot;: 30          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\nterm 和 match 的区别\n两者都是匹配某个属性的值\n全文检索字段用 match\n其他非 text 字段匹配用 term\n\n聚合aggs# 搜索 address 中包含 mill 的所有人的年龄分布以及平均年龄GET bank/_search&#123;  &quot;query&quot;: &#123;    &quot;match&quot;: &#123;      &quot;address&quot;: &quot;mill&quot;    &#125;  &#125;,  &quot;aggs&quot;: &#123;    &quot;ageAgg&quot;: &#123;      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;age&quot;,        &quot;size&quot;: 10      &#125;    &#125;,    &quot;ageAvg&quot;: &#123;      &quot;avg&quot;: &#123;        &quot;field&quot;: &quot;age&quot;      &#125;    &#125;  &#125;&#125;\n\nGET bank/_search&#123;  &quot;query&quot;: &#123;    &quot;match_all&quot;: &#123;&#125;  &#125;,  &quot;size&quot;: 0,   &quot;from&quot;: 0,   &quot;aggs&quot;: &#123;    &quot;ageAgg&quot;: &#123;      &quot;terms&quot;: &#123;        &quot;field&quot;: &quot;age&quot;,        &quot;size&quot;: 10      &#125;,      &quot;aggs&quot;: &#123;        &quot;genderAgg&quot;: &#123;          &quot;terms&quot;: &#123;            &quot;field&quot;: &quot;gender.keyword&quot;,            &quot;size&quot;: 10          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\nik分词器下载链接\nhttps://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v6.7.1\n整合SpringBootpackage com.gxx.es;import com.alibaba.fastjson.JSON;import com.gxx.es.pojo.EsBankParam;import com.gxx.es.pojo.EsUserParam;import io.swagger.annotations.ApiOperation;import org.elasticsearch.action.index.IndexRequest;import org.elasticsearch.action.search.SearchRequest;import org.elasticsearch.action.search.SearchResponse;import org.elasticsearch.client.RequestOptions;import org.elasticsearch.client.RestHighLevelClient;import org.elasticsearch.common.xcontent.XContentType;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.search.SearchHit;import org.elasticsearch.search.aggregations.AggregationBuilders;import org.elasticsearch.search.aggregations.Aggregations;import org.elasticsearch.search.aggregations.bucket.terms.Terms;import org.elasticsearch.search.aggregations.metrics.avg.Avg;import org.elasticsearch.search.aggregations.metrics.max.Max;import org.elasticsearch.search.builder.SearchSourceBuilder;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import java.io.IOException;@RunWith(SpringRunner.class)@SpringBootTestpublic class EsApplicationTests &#123;    @Autowired    private RestHighLevelClient client;    @ApiOperation(&quot;查询测试&quot;)    @Test    public void contextLoads() throws IOException &#123;        SearchRequest request = new SearchRequest();        // 指定索引        request.indices(&quot;bank&quot;);        // 构建查询条件        SearchSourceBuilder builder = new SearchSourceBuilder();        // 多个 query 之间是 or 关系        builder.query(QueryBuilders.matchQuery(&quot;address&quot;, &quot;mill&quot;))                // 字段 address 和 lastname 中有一个包含 &quot;Celeste&quot; 或 &quot;Court&quot; 都满足要求                .query(QueryBuilders.multiMatchQuery(&quot;Celeste Court&quot;, &quot;address&quot;, &quot;lastname&quot;))                // boolQuery 里面的条件是 and 关系，必须全部满足才匹配                .query(QueryBuilders.boolQuery()                        // 必须满足某种条件                        .must(QueryBuilders.matchQuery(&quot;gender&quot;, &quot;F&quot;))                        // 必须不满足某种条件                        .mustNot(QueryBuilders.matchQuery(&quot;gender&quot;, &quot;M&quot;))                        // 对结果进行过滤，没有得分                        .filter(QueryBuilders.matchQuery(&quot;state&quot;, &quot;AK&quot;))                        // 无论满不满足都能查出来，只不过影响得分                        .should(QueryBuilders.termQuery(&quot;age&quot;, 28)))                // 第一个参数是查询结果包含哪些字段，第二个参数是查询结果不包含哪些字段                .fetchSource(null, new String[]&#123;&quot;&quot;&#125;)                .from(2)                .size(10)                // 聚合，统计结果数据的年龄分布                .aggregation(AggregationBuilders.terms(&quot;ageAgg&quot;).field(&quot;age&quot;)                        // 对每一个年龄分布进行进一步的聚合，统计每个年龄分布的性别分布                        .subAggregation(AggregationBuilders.terms(&quot;genderAgg&quot;).field(&quot;gender.keyword&quot;)))                .aggregation(AggregationBuilders.avg(&quot;ageAvg&quot;).field(&quot;age&quot;))                .aggregation(AggregationBuilders.max(&quot;ageMax&quot;).field(&quot;age&quot;));        request.source(builder);        SearchResponse response = client.search(request, RequestOptions.DEFAULT);        // 输出查询结果数据        for (SearchHit hit : response.getHits().getHits()) &#123;            String sourceAsString = hit.getSourceAsString();            EsBankParam esBankParam = JSON.parseObject(sourceAsString, EsBankParam.class);            System.out.println(esBankParam.toString());        &#125;        // 输出聚合结果数据        Aggregations aggregations = response.getAggregations();        Terms ageAgg = aggregations.get(&quot;ageAgg&quot;);        Avg ageAvg = aggregations.get(&quot;ageAvg&quot;);        Max ageMax = aggregations.get(&quot;ageMax&quot;);        ageAgg.getBuckets().forEach(temp -&gt; &#123;            System.out.println(temp.getKeyAsString() + &quot; : &quot; + temp.getDocCount());            Terms genderAgg = temp.getAggregations().get(&quot;genderAgg&quot;);            genderAgg.getBuckets().forEach(t -&gt; System.out.println(t.getKeyAsString() + &quot; : &quot; + t.getDocCount()));        &#125;);        System.out.println(ageAvg.getValue());        System.out.println(ageMax.value());    &#125;    @ApiOperation(&quot;添加测试&quot;)    @Test    public void test01() throws IOException &#123;        EsUserParam userParam = new EsUserParam();        userParam.setId(1);        userParam.setName(&quot;张三&quot;);        IndexRequest indexRequest = new IndexRequest(&quot;user&quot;);        indexRequest.id(String.valueOf(userParam.getId()))                .type(&quot;_doc&quot;)                .source(JSON.toJSONString(userParam), XContentType.JSON);        client.index(indexRequest, RequestOptions.DEFAULT);    &#125;&#125;\n","categories":["中间件","ElasticSearch"],"tags":["ElasticSearch","中间件"]},{"title":"ElasticSearch 父子文档","url":"/2022/01/12/ElasticSearch%E7%88%B6%E5%AD%90%E6%96%87%E6%A1%A3/","content":"参考博文：https://segmentfault.com/a/1190000039964887?utm_source=tag-newest\n介绍父子文档用来处理一对多的映射关系（即一个父文档可以对应多个子文档，但是一个子文档只能对应一个父文档）。如：一个问题有多个答案、一本书籍有多个评论等等。此处我们可以使用 es 的 join数据类型或 nested来实现。此处我们使用join来建立es中的父子文档关系。\n\n每一个mapping下只能有一个join类型的字段。\n父文档和子文档必须在同一个分片(shard)上。即： 增删改查一个子文档都必须和父文档使用相同的 routing key。\n每个元素只能有一个父，但是可以存在多个子。\n可以为一个已经存在的 join 字段增加新的关联关系。\n可以为一个已经是父的元素增加一个子元素。\n\n\njoin数据类型在elasticsearch中不应该像关系型数据库那种使用。而且has_child和has_parent都是比较消耗性能的。\n只有当子的数据远远大于父的数据时，使用join才是有意义的。比如：一个博客下，有多个评论。\n\n需求我们需要创建一个计划(plan)，计划下存在活动(activity)和书籍(book)，书籍下存在评论(comments)。\n即层级结构为：\n     plan    /    \\   /      \\activity  book           |           |          comments\n\n创建映射关系PUT plan_index&#123;  &quot;settings&quot;: &#123;    &quot;number_of_shards&quot;: 3,    &quot;number_of_replicas&quot;: 1  &#125;,  &quot;mappings&quot;: &#123;    &quot;_doc&quot;: &#123;      &quot;properties&quot;: &#123;        &quot;plan_id&quot;:&#123;          &quot;type&quot;: &quot;keyword&quot;        &#125;,        &quot;plan_name&quot;:&#123;          &quot;type&quot;: &quot;text&quot;,          &quot;fields&quot;: &#123;            &quot;keyword&quot;:&#123;              &quot;type&quot; : &quot;keyword&quot;,              &quot;ignore_above&quot; : 256            &#125;          &#125;        &#125;,        &quot;act_id&quot;:&#123;          &quot;type&quot;: &quot;keyword&quot;        &#125;,        &quot;act_name&quot;:&#123;          &quot;type&quot;: &quot;text&quot;,          &quot;fields&quot;: &#123;            &quot;keyword&quot;:&#123;              &quot;type&quot; : &quot;keyword&quot;,              &quot;ignore_above&quot; : 256            &#125;          &#125;        &#125;,        &quot;comment_id&quot;:&#123;          &quot;type&quot;: &quot;keyword&quot;        &#125;,        &quot;comment_name&quot;:&#123;          &quot;type&quot;: &quot;text&quot;,          &quot;fields&quot;: &#123;            &quot;keyword&quot;:&#123;              &quot;type&quot; : &quot;keyword&quot;,              &quot;ignore_above&quot; : 256            &#125;          &#125;        &#125;,        &quot;creator&quot;:&#123;          &quot;type&quot;: &quot;keyword&quot;        &#125;,        &quot;create_time&quot;:&#123;          &quot;type&quot;: &quot;date&quot;,          &quot;format&quot;: &quot;yyyy-MM-dd||yyyy-MM-dd HH:mm:ss&quot;        &#125;,        // 设置一个父子文档关系名        &quot;plan_join&quot;: &#123;          &quot;type&quot;: &quot;join&quot;,          &quot;relations&quot;: &#123;              // plan是父文档，activity、book是子文档            &quot;plan&quot;: [&quot;activity&quot;, &quot;book&quot;],              // book是父文档，comments是子文档            &quot;book&quot;: &quot;comments&quot;          &#125;        &#125;      &#125;    &#125;  &#125;&#125;\n\n添加父文档数据PUT /plan_index/_doc/plan-001&#123;  &quot;plan_id&quot;: &quot;plan-001&quot;,  &quot;plan_name&quot;: &quot;四月计划&quot;,  &quot;creator&quot;: &quot;huan&quot;,  &quot;create_time&quot;: &quot;2021-04-07 16:27:30&quot;,  &quot;plan_join&quot;: &#123;    &quot;name&quot;: &quot;plan&quot;  &#125;&#125;PUT /plan_index/_doc/plan-002&#123;  &quot;plan_id&quot;: &quot;plan-002&quot;,  &quot;plan_name&quot;: &quot;五月计划&quot;,  &quot;creator&quot;: &quot;huan&quot;,  &quot;create_time&quot;: &quot;2021-05-07 16:27:30&quot;,  &quot;plan_join&quot;: &quot;plan&quot;&#125;\n\n注意⚠️：\n1、如果是创建父文档，则需要使用 plan_join 指定父文档的关系的名字(此处为plan)。\n2、plan_join为创建索引的 mapping时指定join的字段的名字。\n3、指定父文档时,plan_join的这2种写法都可以。\n添加子文档PUT /plan_index/_doc/act-001?routing=plan-001&#123;  &quot;act_id&quot;:&quot;act-001&quot;,  &quot;act_name&quot;:&quot;四月第一个活动&quot;,  &quot;creator&quot;:&quot;huan.fu&quot;,  &quot;plan_join&quot;:&#123;    &quot;name&quot;:&quot;activity&quot;,    &quot;parent&quot;:&quot;plan-001&quot;  &#125;&#125;PUT /plan_index/_doc/book-001?routing=plan-001&#123;  &quot;book_id&quot;:&quot;book-001&quot;,  &quot;book_name&quot;:&quot;四月读取的第一本书&quot;,  &quot;creator&quot;:&quot;huan.fu&quot;,  &quot;plan_join&quot;:&#123;    &quot;name&quot;:&quot;book&quot;,    &quot;parent&quot;:&quot;plan-001&quot;  &#125;&#125;PUT /plan_index/_doc/book-002?routing=plan-001&#123;  &quot;book_id&quot;:&quot;book-002&quot;,  &quot;book_name&quot;:&quot;编程珠玑&quot;,  &quot;creator&quot;:&quot;huan.fu&quot;,  &quot;plan_join&quot;:&#123;    &quot;name&quot;:&quot;book&quot;,    &quot;parent&quot;:&quot;plan-001&quot;  &#125;&#125;PUT /plan_index/_doc/book-003?routing=plan-002&#123;  &quot;book_id&quot;:&quot;book-003&quot;,  &quot;book_name&quot;:&quot;java编程思想&quot;,  &quot;creator&quot;:&quot;huan.fu&quot;,  &quot;plan_join&quot;:&#123;    &quot;name&quot;:&quot;book&quot;,    &quot;parent&quot;:&quot;plan-002&quot;  &#125;&#125;# 理论上 comment 的父文档是 book ，但是此处routing使用 plan 也是可以的。PUT /plan_index/_doc/comment-001?routing=plan-001&#123;  &quot;comment_id&quot;:&quot;comment-001&quot;,  &quot;comment_name&quot;:&quot;这本书还可以&quot;,  &quot;creator&quot;:&quot;huan.fu&quot;,  &quot;plan_join&quot;:&#123;    &quot;name&quot;:&quot;comments&quot;,    &quot;parent&quot;:&quot;book-001&quot;  &#125;&#125;PUT /plan_index/_doc/comment-002?routing=plan-001&#123;  &quot;comment_id&quot;:&quot;comment-002&quot;,  &quot;comment_name&quot;:&quot;值得一读，棒。&quot;,  &quot;creator&quot;:&quot;huan.fu&quot;,  &quot;plan_join&quot;:&#123;    &quot;name&quot;:&quot;comments&quot;,    &quot;parent&quot;:&quot;book-001&quot;  &#125;&#125;\n\n注意⚠️：\n\n\n子文档（子孙文档等）需要和父文档使用相同的路由键。\n需要指定父文档的id。\n需要指定join的名字。\n\n查询文档根据父文档id查询它下方的子文档// 返回父文档id是plan-001下的类型为book的所有子文档。GET /plan_index/_search&#123;  &quot;query&quot;:&#123;    &quot;parent_id&quot;: &#123;      &quot;type&quot;:&quot;book&quot;,      &quot;id&quot;:&quot;plan-001&quot;    &#125;  &#125;&#125;\n\n\nhas_child返回满足条件的父文档// 返回创建者(creator)是huan.fu,并且子文档最少有2个的父文档。GET /plan_index/_search&#123;  &quot;query&quot;: &#123;    &quot;has_child&quot;: &#123;      &quot;type&quot;: &quot;book&quot;,      &quot;min_children&quot;: 2,        &quot;query&quot;: &#123;        &quot;match&quot;: &#123;          &quot;creator&quot;: &quot;huan.fu&quot;        &#125;      &#125;    &#125;  &#125;&#125;\n\n\nhas_parent返回满足父文档的子文档// 返回父文档(book)的创建者是huan.fu的所有子文档GET /plan_index/_search&#123;  &quot;query&quot;: &#123;    &quot;has_parent&quot;: &#123;      &quot;parent_type&quot;: &quot;book&quot;,      &quot;query&quot;: &#123;        &quot;match&quot;: &#123;          &quot;creator&quot;:&quot;huan.fu&quot;        &#125;      &#125;    &#125;  &#125;&#125;\n\n\nNested Object 和 join 对比\n\n\nNested Object\njoin (Parent/Child)\n\n\n\n文档存储在一起，读取性能高\n父子文档单独存储，互不影响。但是为了维护join的关系，需要占用额外的内容，读取性能略差。\n\n\n更新父文档或子文档时，需要更新整个文档。\n父文档和子文档可以单独更新。\n\n\n适用于查询频繁，子文档偶尔更新的情况。\n适用于更新频繁的情况，且子文档的数量远远超过父文档的数量。\n\n\nSpringBoot中父子文档的使用@ApiOperation(&quot;父子文档测试&quot;)@Testpublic void test02() throws IOException &#123;    SearchRequest request = new SearchRequest();    // 指定索引    request.indices(&quot;plan_index    // 构建查询条件    SearchSourceBuilder builder = new SearchSourceBuilder();    builder.query(JoinQueryBuilders.parentId(&quot;book&quot;, &quot;plan-001&quot;));    builder.query(JoinQueryBuilders.hasChildQuery(&quot;book&quot;, QueryBuilders.matchQuery(&quot;creator&quot;, &quot;huan.fu&quot;), ScoreMode.Min));    builder.query(JoinQueryBuilders.hasParentQuery(&quot;plan&quot;, QueryBuilders.matchQuery(&quot;creator&quot;, &quot;huan.fu&quot;), true));    request.source(builder);    SearchResponse response = client.search(request, RequestOptions.DEFAUL    // 输出查询结果数据    for (SearchHit hit : response.getHits().getHits()) &#123;        String sourceAsString = hit.getSourceAsString();        PlanEntity bookEntity = JSON.parseObject(sourceAsString, PlanEntity.class);        System.out.println(bookEntity.toString());    &#125;&#125;                                        @ApiOperation(&quot;添加父子文档测试&quot;)@Testpublic void test03() throws IOException &#123;    Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();    Map&lt;String, String&gt; plan_join = new HashMap&lt;&gt;();    plan_join.put(&quot;name&quot;, &quot;book&quot;);    plan_join.put(&quot;parent&quot;, &quot;plan-003&quot;);    map.put(&quot;book_id&quot;, &quot;book-009&quot;);    map.put(&quot;book_name&quot;, &quot;Python&quot;);    map.put(&quot;creator&quot;, &quot;zhong&quot;);    map.put(&quot;create_time&quot;, &quot;2021-04-07 16:27:30&quot;);    map.put(&quot;plan_join&quot;, plan_join);    IndexRequest indexRequest = new IndexRequest(&quot;plan_index&quot;);    indexRequest.id((String) map.get(&quot;book_id&quot;))            .routing(&quot;plan-003&quot;)            .type(&quot;_doc&quot;)            .source(new JSONObject(map), XContentType.JSON);    client.index(indexRequest, RequestOptions.DEFAULT);&#125;\n","categories":["中间件","ElasticSearch"],"tags":["ElasticSearch","中间件"]},{"title":"Git - 学习笔记","url":"/2021/11/21/Git-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","content":"观看尚硅谷视频自己整合的笔记\n视频地址：【尚硅谷】5h打通Git全套教程IDEA版（涵盖GitHub\\Gitee码云\\GitLab）_哔哩哔哩_bilibili\nGit 概述Git 是一个免费的、开源的分布式版本控制系统，可以快速高效地处理从小型到大型的各种项目。\nGit 易于学习，占地面积小，性能极快。它具有廉价的本地库，方便的暂存区域和多个工作流分支等特性。其性能优于 Subversion、CVS、Perforce 和 ClearCase 等版本控制工具。\n何为版本控制版本控制是一种记录文件内容变化、以便将来查阅特定版本修订情况的系统。\n版本控制其实最重要的是可以记录文件修改历史纪录，从而让用户能够查看历史版本，方便版本切换。\n\n\n\n版本控制工具集中式版本控制工具\nCVS、SVN(Subversion)、VSS……\n集中化的版本控制系统诸如 CVS、SVN 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。多年以来，这已成为版本控制系统的标准做法。这种做法带来了许多好处每个人都可以在一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控个开发者的权限，并且管理一个集中化的版本控制系统，要远比在各个客户端上维护本地数据库来得轻松容易。事分两面，有好有坏。这么做显而易见的缺点是中央服务器的单点障。如果服务器宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。\n\n\n分布式版本控制工具\nGit、Mercurial、Bazaar、Darcs……\n像 Git 这种分布式版本控制工具，客户端提取的不是最新版本的文件快照，而是把代码仓库完整地镜像下来（本地库）。这样任何一处协同工作用的文件发生故障，事后都可以用其他客户端的本地仓库进行恢复。因为每个客户端的每一次文件提取操作，实际上都是一次对整个文件仓库的完整备份。分布式的版本控制系统出现之后,解决了集中式版本控制系统的缺陷:\n\n服务器断网的情况下也可以进行开发（因为版本控制是在本地进行的）\n每个客户端保存的也都是整个完整的项目（包含历史记录，更加安全）\n\n\n\nGit 工作机制\n\n工作区\n在某个路径下执行 git init 命令初始化（或者通过克隆的方式）之后，都会生成一个 .git 的隐藏文件夹，那么这些我们可以直接修改的文件的所在空间就是工作区：\n\n\n暂存区\n版本库里面的关键，暂存区，当我们使用 git add [文件名] 或者 git add . 命令的时候，就是把我们在工作区添加、删除或者修改等等在工作区对文件的操作，同步到暂存区（暂存区的内容变得跟工作区一样）\n本地库\n我们在本地上复制一份网络上的仓库，把它作为本地仓库并对它进行操作，最后提交到网络上面。\nGit 和代码托管中心代码托管中心是基于网络服务器的远程代码仓库，一般我们简单称为远程库。\n局域网\n\nGitLab\n\n互联网\n\nGitHub（外网）\nGitee 码云（国内网站）\n\nGit 常用命令\n\n\n命令名称\n作用\n\n\n\ngit config –global user.name 用户名\n设置用户签名\n\n\ngit config –global user.email 邮箱\n设置用户签名\n\n\ngit init\n初始化本地库\n\n\ngit status\n查看本地库状态\n\n\ngit add 文件名\n添加到暂存区\n\n\ngit commit -m “日志信息” 文件名\n提交到本地库\n\n\ngit reflog\n查看历史记录\n\n\ngit reset –hard 版本号\n版本穿梭\n\n\n设置用户签名\n安装后只需要设置一次即可，后面使用无须再设置\n\n初始化本地库\n看不到 .git 文件的需要设置成可查看隐藏文件\n\n查看本地库状态\n先使用 vim test 命令创建并编辑 test 文本，添加一些信息并保存后，再使用 git status 查看本地库状态\n文件为红色表示存在于工作区\n\n添加到暂存区\n使用 git add 命令将工作区的文件添加到暂存区\n文件为绿色表示存在于暂存区\n\n删除暂存区文件\n使用 git rm –cached [文件名] 命令删除暂存区的文件\n删除后重新变为红色，说明工作区的文件并没有被删除\n\n提交到本地库\n使用 git commit -m “日志信息” 文件名 命令将暂存区的文件提交到本地仓库\n \n 查看提交日志\n\n 修改文件内容后再次提交到本地库\n\n 此时指针指向第二次提交的版本\n\n版本穿梭\n使用 git reset –hard 版本号 切换版本\n下图为从第二次提交的版本切换到第一次提交的版本\n切换后文件内容就会变为切换后版本的内容\n\nGit 分支操作什么是分支在版本控制过程中，同时推进多个任务，为每个任务，我们就可以创建每个任务的单独分支。使用分支意味着程序员可以把自己的工作从开发主线上分离开来，开发自己分支的时候，不会影响主线分支的运行。对于初学者而言，分支可以简单理解为副本，一个分支就是一个单独的副本。（分支底层其实也是指针的引用）\n\n\n分支的好处\n同时并行推进多个功能开发，提高开发效率。\n各个分支在开发过程中，如果某一个分支开发失败，不会对其他分支有任何影响。失败的分支删除重新开始即可。\n\n分支命令\n\n\n命令名称\n作用\n\n\n\ngit branch 分支名\n创建分支\n\n\ngit branch -v\n查看分支\n\n\ngit checkout 分支名\n切换分支\n\n\ngit merge 分支名\n把指定的分支合并到当前分支\n\n\n查看&amp;创建分支\n\n查看分支：git branch -v\n创建分支：git branch [分支名]\n\n\n切换分支\n\n命令：gitcheckout [分支名]\n\n\n合并分支（正常合并）\n\n命令：git merge [分支名]\n\nmaster 分支的 test 文本内容\n\n hot-fix 分支的 test 文本内容\n\n 在 master 分支上合并 hot-fix 分支\n\n合并分支（冲突合并）\n冲突产生的原因：\n合并分支时，两个分支在同一个文件的同一个位置有两套完全不同的修改。Git 无法替我们决定使用哪一个，必须人为决定新代码内容\n尝试在 master 分支上合并 hot-fix 分支\n\n手动解决冲突\n\nvim 命令打开冲突文本\n\n\n\n\n把文本内容修改成自己想要的内容后保存\n\n添加暂存区 =》提交本地库 =》合并完成\n\n\n\n\n创建分支和切换分支图解\nmaster、hot-fix 其实都是指向具体版本记录的指针。当前所在的分支，其实是由 HEAD决定的。所以创建分支的本质就是多创建一个指针。\nHEAD 如果指向 master，那么我们现在就在 master 分支上。\nHEAD 如果执行 hot-fix，那么我们现在就在 hot-fix 分支上。\n\n结论：切换分支的本质就是移动 HEAD 指针。\n","categories":["Git"],"tags":["Git"]},{"title":"Linux 上部署前后端分离项目","url":"/2021/12/06/Linux%20%E4%B8%8A%E9%83%A8%E7%BD%B2%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%E9%A1%B9%E7%9B%AE/","content":"技术\n前端：Vue\n后端：SpringBoot\n服务器：Centos 7\n\n步骤1、打包 SpringBoot 项目在 idea 上打开你的 SpringBoot 项目，然后再打开控制台 Terminal，输入如下命令\nmvn package\n\n注意：打包之前确保当前目录是你要打包的项目目录\n控制台出现以下信息说明打包成功！\n\n\n还有一种更简单的方法就是直接使用 Maven 工具进行可视化操作的打包，如下图\n点击 idea 右侧的 Maven 按钮，然后再双击 package 即可完成打包\n\n\n出现如下信息说明打包完成\n\n\n打包完成后就会在项目的 target 文件里面生成一个 jar 包，这个 jar 包就是后面用来放到 Linux 上启动的。\n2、打包 Vue 项目在 Vue 项目上打开控制台 Terminal，然后执行如下命令进行打包\n注意：在打包之前，需要将访问后端的 ip 路径改成 Linux 的 ip 地址\nnpm run build\n\n控制台输出以下信息说明打包成功！\n\n\n打包成功后就会在 Vue 项目的目录下生成一个 dist 文件夹，后面需要将这个文件夹里面的所有内容都放到 nginx 上。\n\n\n3、Linux 部署环境SpringBoot 启动需要有 JDK 环境，前端部署需要安装 Nginx。\nJDK 安装\n# 执行如下命令安装 JDK1.8yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel# 查看 JDK 版本java -version\n\n安装成功 \n\n Nginx 安装教程：通俗易懂 - 手把手教你在 Linux（Centos 7） 上安装 Nginx_m0_61587348的博客-CSDN博客\n4、SpringBoot 项目部署启动把步骤1打包生成的 jar 包传送到 Linux 服务器上，然后在 Linux 上执行如下命令进行启动（执行命令前确保当前路径位置为 jar 存放的位置）\n# nohup 意思是不挂断运行命令,当账户退出或终端关闭时,程序仍然运行# 当用 nohup 命令执行作业时，缺省情况下该作业的所有输出被重定向到nohup.out的文件中# 除非另外指定了输出文件nohup java -jar jar包名 &amp;\n\n查看启动情况\n# 7001 换成你的 SpringBoot 程序启动的端口号netstat -nlp | grep : 7001\n\n有一些 Linux 没有安装这个命令的，执行如下命令进行安装\nyum install -y net-tools\n\n启动成功\n\n 想要停止程序也很简单\n# 注意：7315 是上图右侧的数字而不是端口号kill 7315\n\n5、Nginx 配置&amp; Vue 项目部署启动步骤3中我们已经成功安装了 Nginx，并且安装位置为 /usr/local/nginx\n现在要将步骤2中打包生成的 dist 文件夹下的所有内容整合到 Nginx 上\n进入 Nginx 的 html 目录下\ncd /usr/local/nginx/html\n\n然后将 html 目录下的所有内容替换为 dist 文件夹里面的内容\n最后启动 Nginx\n# 进入 nginx 目录cd /usr/local/nginx# 启动 nginx./sbin/nginx\n\n6、部署完成在浏览器输入 Linux 系统的 IP 地址，如果能够正常显示前端页面并且能正常访问后端接口，说明部署成功！\n记得关防火墙或者开放相应的端口号！！！\n记得关防火墙或者开放相应的端口号！！！\n记得关防火墙或者开放相应的端口号！！！\n","categories":["SpringBoot"],"tags":["SpringBoot","Linux","Vue"]},{"title":"Spring Boot 利用 AOP 自定义注解实现日志功能","url":"/2022/01/07/Spring%20Boot%20%E5%88%A9%E7%94%A8%20AOP%20%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E5%8A%9F%E8%83%BD/","content":"需求描述在日常开发中，日志是项目开发必不可少的一部分。这里我使用 Spring Boot 利用 AOP 和自定义注解的方式实现日志功能。这样只需要在某一个类、方法或者接口上添加一个我们自定义的注解，就可以利用 AOP 原理在这个类、方法或者接口的前后记录一些日志。\n代码实现项目结构图\nMaven 依赖&lt;dependencies&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;        &lt;version&gt;2.2.12.RELEASE&lt;/version&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;version&gt;2.2.12.RELEASE&lt;/version&gt;    &lt;/dependency&gt;&lt;/dependencies&gt;\n\n自定义注解（Log）注解作用\n@Documented\n\n​    作用：注解可以被javadoc此类的工具文档化，只负责标记，没有成员取值\n\n@Target\n\n​    作用：注解的生存周期（会保留到哪个阶段），使用枚举类： ElementType声明，主要有：\nTYPE（类、接口（包括注解类型）或枚举声明）FIELD（字段声明）METHOD（方法声明）PARAMETER（形参声明）CONSTRUCTOR（构造函数声明）LOCAL_VARIABLE（局部变量声明）ANNOTATION_TYPE（注解类型声明）PACKAGE（包装声明）TYPE_PARAMETER（类型参数声明）TYPE_USE（类型的使用）\n\n\n@Retention\n\n​    作用：注解的使用范围（被描述的注解可以用在哪里），使用枚举类：RetentionPolicy 声明，一共有三种：\nSOURCE：源码级别保留，编译后即丢弃CLASS:编译级别保留，编译后的class文件中存在，在jvm运行时丢弃，这是默认值。RUNTIME： 运行级别保留，编译后的class文件中存在，在jvm运行时保留，可以被反射调用。\n\n代码实现package com.abc.log.annotaion;import java.lang.annotation.*;@Documented@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Log &#123;    int type() default 0;    String value() default &quot;&quot;;&#125;\n\n切面类（LogAspect）==记得添加@Aspect注解，表示这是一个切面类==\npackage com.abc.log.aspect;import com.abc.log.annotaion.Log;import org.aspectj.lang.ProceedingJoinPoint;import org.aspectj.lang.Signature;import org.aspectj.lang.annotation.Around;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.Pointcut;import org.aspectj.lang.reflect.MethodSignature;import org.springframework.stereotype.Component;@Aspect@Componentpublic class LogAspect &#123;    /**     * 设置切点表达式（这里是对 Log 注解进行增强）     */    @Pointcut(&quot;@annotation(com.abc.log.annotaion.Log)&quot;)    public void log() &#123;&#125;    /**     * 采用环绕通知的方式     */    @Around(value = &quot;log()&quot;)    public Object doAround(ProceedingJoinPoint joinPoint) &#123;        // 获取 Log 注解的两个值        MethodSignature signature = (MethodSignature)joinPoint.getSignature();        Log annotation = signature.getMethod().getAnnotation(Log.class);        int type = annotation.type();        String value = annotation.value();        // 方法返回结果        Object result = null;        try &#123;            System.out.println(&quot;日志类型：&quot; + type + &quot; 日志描述：&quot; + value + &quot; 方法执行前的日志打印......&quot;);            // 执行被增强的方法            result = joinPoint.proceed();            System.out.println(&quot;日志类型：&quot; + type + &quot; 日志描述：&quot; + value + &quot; 方法执行结果：&quot; + result);        &#125; catch (Throwable throwable) &#123;            System.out.println(&quot;日志类型：&quot; + type + &quot; 日志描述：&quot; + value + &quot; 方法执行异常的日志打印......&quot;);            throwable.printStackTrace();        &#125; finally &#123;            System.out.println(&quot;日志类型：&quot; + type + &quot; 日志描述：&quot; + value + &quot; 方法执行完成的日志打印......&quot;);        &#125;        return result;    &#125;&#125;\n\nController类package com.abc.log.controller;import com.abc.log.annotaion.Log;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/log&quot;)public class LogController &#123;    @Log(type = 1, value = &quot;测试&quot;)    @GetMapping(&quot;/ok&quot;)    public String logTest() &#123;        System.out.println(&quot;方法执行中......&quot;);        return &quot;ok&quot;;    &#125;&#125;\n\n测试在浏览器输入url：http://localhost:8080/log/ok\n浏览器显示结果：\n控制台显示结果：\n\n","categories":["SpringBoot"],"tags":["SpringBoot","注解","AOP"]},{"title":"从 Java 虚拟机字节码角度分析 i++ 与 ++i 的区别","url":"/2022/01/09/%E4%BB%8E-Java-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%97%E8%8A%82%E7%A0%81%E8%A7%92%E5%BA%A6%E5%88%86%E6%9E%90-i-%E4%B8%8E-i-%E7%9A%84%E5%8C%BA%E5%88%AB/","content":"前言相信学过 Java 的人都知道，i++ 就是先拿出来使用然后再 +1，而 ++i 就是先 +1 再拿出来使用。但是其实很多人并不知道其内部的实现原理，只是知道结果是这样。下面就从 Java 虚拟机字节码的角度分几种情况来对 i++ 与 ++i 做一个详细的对比。\n前置知识介绍局部变量表局部变量表是用来存储一个方法的方法参数以及局部变量的，可以想象成一个数组，所有的参数以及局部变量都按顺序存放在局部变量表中。至于谁先谁后，就取决于这些变量在方法中谁先定义了。我们在代码中看到的方法参数和局部变量的值就是从这个局部变量表中拿的。\n字节码我们在编程工具中写的 Java 代码是不能直接被 Java 虚拟机识别的，所以 Java 代码在程序编译的时候会被编译为字节码，然后再加载到 Java 虚拟机中运行。\n操作数栈主要用于保存计算过程中的中间结果，同时作为计算过程中变量临时的存储空间。\n例子Demo1public class Demo01 &#123;    public static void main(String[] args) &#123;        int a = 0;        int b = 0;        a++;        ++b;    &#125;&#125;\n\n编译后的字节码如下\n0 iconst_0        # 定义一个常量0（代表a）1 istore_1        # 把常量0放入局部变量表下标为1的位置2 iconst_0        # 再定义一个常量0（代表b）3 istore_2        # 把常量0放入局部变量表下标为2的位置4 iinc 1 by 1     # 对存放在局部变量下标为1的那个常量0做+1操作（就是对局部变量表中的a加1，此时a就变为了1了）7 iinc 2 by 1     # 对存放在局部变量下标为2的那个常量0做+1操作（就是对局部变量表中的b加1，此时b就变为了1了）10 return         # 程序结束\n\n结论：通过上面对字节码的分析可以看出，如果一行中只有 i++ 或者 ++i，那么它们两个的作用是一样的（都是+1），因为它们的字节码是一模一样的。\nDemo2public class Demo02 &#123;    public static void main(String[] args) &#123;        int a = 0;        int b = 0;        int c = 0;        int d = 0;        c = a++;    // 结果为: 0        d = ++b;    // 结果为: 1    &#125;&#125;\n\n 编译后的字节码如下\n 0 iconst_0    # 定义一个常量0（代表a） 1 istore_1    # 把常量0放入局部变量表下标为1的位置 2 iconst_0    # 定义一个常量0（代表b） 3 istore_2    # 把常量0放入局部变量表下标为2的位置 4 iconst_0    # 定义一个常量0（代表c） 5 istore_3    # 把常量0放入局部变量表下标为3的位置 6 iconst_0    # 定义一个常量0（代表d） 7 istore 4    # 把常量0放入局部变量表下标为4的位置 9 iload_1     # 把a从局部变量表中加载一份到操作数栈中（此时操作数栈中的a为0）10 iinc 1 by 1 # 在局部变量表中对a进行+1操作（此时局部变量表中a的值为1）13 istore_3    # 最后把操作数栈中的a放回到局部变量表下标为3的位置（其实就是将a赋值给c，此时局部变量中的c就变为0）14 iinc 2 by 1 # 在局部变量表中对b进行+1操作（此时局部变量表中b的值为1）17 iload_2     # 把b从局部变量表中加载一份到操作数栈中（此时操作数栈中的b为1）18 istore 4    # 最后把操作数栈中的b放回到局部变量表下标为4的位置（其实就是将b赋值给d，此时局部变量中的d就变为1）20 return      # 程序结束\n\n结论：从字节码角度可以看出，c = a++ 就是先把还没 +1 的 a 存一份，然后对 a 进行 +1 操作，最后再把存的那一份 a 赋值给 c，所以我们看到的结果 c = 0；而 d = b++ 就是先对 b 进行 +1 操作，然后再 +1 后的 b 存一份，最后把存的这一份赋值给 d，所以我们看到的结果是 d = 1。\nDemo3public class Demo03 &#123;    public static void main(String[] args) &#123;        int a = 0;        int b = 0;        a = a++;    // 结果为 0        b = ++b;    // 结果为 1    &#125;&#125;\n\n编译后的字节码如下\n 0 iconst_0        # 定义一个常量0（代表a） 1 istore_1        # 把常量0放入局部变量表下标为1的位置 2 iconst_0        # 再定义一个常量0（代表b） 3 istore_2        # 把常量0放入局部变量表下标为2的位置 4 iload_1         # 加载局部变量表下标为1的变量（a）到操作数栈中（此时操作数栈中a的值为0） 5 iinc 1 by 1     # 对存放在局部变量下标为1的那个常量0做+1操作（此时局部变量表中的a就变为了1了） 8 istore_1        # 再把加载到操作数栈中的变量（a）放回到局部变量表下标为1的位置（此时局部变量表中的a被覆盖，变为0） 9 iinc 2 by 1     # 对存放在局部变量下标为2的那个常量0做+1操作（此时局部变量表中的b就变为了1了）10 iload_2         # 加载局部变量表下标为2的变量（b）到操作数栈中（此时操作数栈中b的值为1）13 istore_2        # 再把加载到操作数栈中的变量（b）放回到局部变量表下标为1的位置（此时局部变量表中的b被覆盖，但是还是为1）14 return          # 程序结束\n\n结论：这个其实跟 Demo2 基本一致，搞清楚 Demo2 之后这个自然就迎刃而解了。\n相信通过着三个案例，大家对 i++ 和 ++i 会有一个更深入的理解，其他的一些情况，大家可以自己去测试一下，有什么不懂的可以在评论区留言哦，大家一起讨论！\n","categories":["JVM"],"tags":["Java","JVM"]},{"title":"使用 Hexo 搭建个人博客","url":"/2022/01/09/%E4%BD%BF%E7%94%A8%20Hexo%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/","content":"前置环境本次博客搭建是在 Linux 上实现的，这里使用的是 Centos 7 系统（理论上哪个版本都一样的）。下面是搭建博客前需要安装的部署环境：\nNode.js# 创建一个文件夹（node.js安装位置）mkdir /usr/local/node.js# 进入到这个文件夹当中cd /usr/local/node.js# 下载node.js安装包wget https://nodejs.org/dist/v16.13.1/node-v16.13.1-linux-x64.tar.xz# 解压tar xf node-v16.13.1-linux-x64.tar.xz# 移动解压出来的文件到node.js目录cp node-v16.13.1-linux-x64/* ./# 修改Linux的环境变量# 先复制一份cp /etc/profile /etc/profile.bak# 然后将下面的语句插入到profile文件的末尾export PATH=$PATH:/usr/local/node.js/bin# 立即生效source /etc/profile# 查看版本node -vv16.13.1\n\nGitsudo yum install git-core\n\n安装 Hexo所有必备的应用程序安装完成后，即可使用 npm 安装 Hexo。\nnpm install -g hexo-cli\n\n安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。\nhexo init &lt;folder&gt;cd &lt;folder&gt;npm install\n\nHexo 相关命令# 启动Hexohexo s -p 80# 关闭Hexo直接 Ctrl + c# 修改相关配置文件后重新启动Hexohexo cleanhexo ghexo s -p 80# 创建一篇博文hexo n &#x27;标题&#x27;\n\n","categories":["博客搭建"],"tags":["Hexo","Blog"]},{"title":"使用 RestTemplate 发送 HTTP 请求","url":"/2022/01/09/%E4%BD%BF%E7%94%A8-RestTemplate-%E5%8F%91%E9%80%81-HTTP-%E8%AF%B7%E6%B1%82/","content":"介绍RestTemplate 是由 Spring 提供的一个 HTTP 请求工具。比传统的 Apache 和 HttpCLient 便捷许多，能够大大提高客户端的编写效率。\n使用getForEntity()\n发送一个 HTTP GET 请求，返回的 ResponseEntity 包含了响应体所映射成的对象\n@Testvoid getForEntity() &#123;    RestTemplate restTemplate = new RestTemplate();    // URL：需要请求的地址  User.class：请求返回的结果类型    PARAM01、PARAM02：请求参数，没有可以不写    ResponseEntity&lt;User&gt; entity = restTemplate.getForEntity(&quot;http://localhost:8080/getUser/&#123;param01&#125;/&#123;param02&#125;&quot;, User.class, PARAM01, PARAM02);    User user = entity.getBody();&#125;\n\ngetForObject()\n发送一个 HTTP GET 请求，返回的请求体将映射为一个对象\n@Testvoid getForObject() &#123;    RestTemplate restTemplate = new RestTemplate();    // URL：需要请求的地址  User.class：请求返回的结果类型    PARAM01、PARAM02：请求参数，没有可以不写    User user = restTemplate.getForObject(&quot;http://localhost:8080/getUser/&#123;param01&#125;/&#123;param02&#125;&quot;, User.class, PARAM01, PARAM02);&#125;\n\npostForEntity()\n发送一个 HTTP POST 请求，返回的 ResponseEntity 包含了响应体所映射成的对象\n@Testvoid postForEntity() &#123;    User user = new User();    RestTemplate restTemplate = new RestTemplate();    // URL：需要请求的地址    user：请求携带的对象  Boolean.class：请求返回的结果类型    PARAM01、PARAM02：请求参数，没有可以不写    ResponseEntity&lt;Boolean&gt; entity = restTemplate.postForEntity(&quot;http://localhost:8080/saveUser/&#123;param01&#125;/&#123;param02&#125;&quot;, user, Boolean.class, PARAM01, PARAM02);    // 获取返回结果    Boolean isSave = entity.getBody();&#125;\n\npostForObject()\n发送一个 HTTP POST 请求，返回的请求体将映射为一个对象\n@Testvoid postForObject() &#123;    User user = new User();    RestTemplate restTemplate = new RestTemplate();    // URL：需要请求的地址    user：请求携带的对象  Boolean.class：请求返回的结果类型    PARAM01、PARAM02：请求参数，没有可以不写    Boolean isSave = restTemplate.postForObject(URL, user, Boolean.class, PARAM01, PARAM02);&#125;\n\nexchange()\n在 URL 上执行特定的 HTTP 方法，返回包含对象的 ResponseEntity，这个对象是从响应体中映射得到的\n@Testvoid exchange() &#123;    RestTemplate restTemplate = new RestTemplate();    HttpHeaders headers = new HttpHeaders();    // 设置请求头信息    headers.setContentType(MediaType.MULTIPART_RELATED);    headers.add(&quot;token&quot;, &quot;abc&quot;);    // 设置请求体信息    MultiValueMap&lt;String, String&gt; map= new LinkedMultiValueMap&lt;&gt;();    map.add(&quot;user&quot;, &quot;abc&quot;);    // URL：需要请求的地址    HttpMethod.GET：请求方式   new HttpEntity&lt;Object&gt;(headers, map)：携带请求头和请求体信息  String.class：请求返回的结果类型    ResponseEntity&lt;String&gt; entity = restTemplate.exchange(URL, HttpMethod.GET, new HttpEntity&lt;Object&gt;(headers, map), String.class);    String body = entity.getBody();&#125;\n\n"},{"title":"在 Linux(Centos 7) 上安装 Nginx","url":"/2022/01/09/%E5%9C%A8-Linux-Centos-7-%E4%B8%8A%E5%AE%89%E8%A3%85-Nginx/","content":"安装前置配置在安装 nginx 之前，我们得先安装一些前置得配置。\n依次执行以下命令\n# gcc-c++ 编译器yum install gcc-c++yum install -y openssl openssl-devel# pcre包yum install -y pcre pcre-devel# zlib包yum install -y zlib zlib-devel\n\n创建安装目录首先在 /usr/local 目录上创建 nginx 目录，下面就在这个目录下安装 nginx\n# 创建 nginx 目录mkdir /usr/local/nginx# 进入 nginx 目录cd /usr/local/nginx\n\n下载 nginx 压缩包官方下载地址：Index of /download/\n下载有两种方法\n\n直接在官网下载，然后再把压缩包上传到 Linux 上\n使用 wget 命令直接在 Linux 上下载\n\n这里我是用了第二种方式\n在 nginx 目录下执行如下命令\n# 我下载的是1.20.1的版本wget https://nginx.org/download/nginx-1.20.1.tar.gz\n\n下载成功后 nginx 目录下就会出现一个压缩包\n\n\n有些网友的 Linux 上可能没有 wget 命令，可执行以下命令进行安装\nyum install wget\n\n解压&amp;配置使用 tar 命令进行解压\ntar -zxvf nginx-1.20.1.tar.gz\n\n进入解压后的 nginx 目录\ncd nginx-1.20.1\n\n使用 nginx 默认配置\n./configure\n\n编译安装# 编译make# 安装make install# 查找安装路径whereis nginx\n\n安装成功会显示安装的路径\n\n启动 nginx# 退回到 nginx 目录cd ..# 启动 nginx./sbin/nginx# 查看 nginx 启动情况ps -ef | grep nginx\n\n\n启动成功在浏览器上输入 Linux 的 ip 地址，敲回车后出现如下页面说明启动成功！\n\n\n在启动前记得关防火墙或开放80端口！！!\n在启动前记得关防火墙或开放80端口！！!\n在启动前记得关防火墙或开放80端口！！!\n其他命令# 重启./sbin/nginx -s reload# 快速停止./sbin/nginx -s stop# 完整有序的停止./sbin/nginx -s quit\n\n以上命令执行的前提是你已经进入了 nginx 目录\ncd /usr/local/nginx","categories":["环境搭建"],"tags":["Linux","环境搭建","Nginx"]},{"title":"Hellow World","url":"/2020/04/07/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n","categories":["Hexo"],"tags":["Hexo"]},{"title":"冒泡排序","url":"/2022/01/09/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/","content":"实现思路通过对待排序序列从前向后（从小标较小的元素开始），依次比较相邻元素的值，若发现逆序则交换，使值较大的元素逐渐从前移向后部，就像水底下的气泡一样逐渐向上冒。\n动态图演示排序过程\n\n复杂度与稳定性\n平均的时间复杂度为：O( n^2 )\n平均的空间复杂度为：O(1)\n冒泡排序是一种稳定的排序算法\n\n代码实现private static void bubbleSort(int[] arr) &#123;    for (int i = arr.length; i &gt; 1; i--) &#123;        for (int j = 0; j &lt; i - 1; j++) &#123;            if (arr[j] &gt; arr[j + 1]) &#123;                int a = arr[j];                arr[j] = arr[j + 1];                arr[j + 1] = a;            &#125;        &#125;    &#125;&#125;\n\n优化思路因为排序的过程中，各元素不断接近自己的位置，如果一趟比较下来没有进行过交换，就说明序列有序，因此要在排序过程中设置一个标志flag判断元素是否进行过交换，从而减少不必要的比较。\n优化后的代码实现public static void bubbleSort(int[] arr) &#123;    for (int i = arr.length; i &gt; 1; i--) &#123;        boolean flag = true;        for (int j = 1; j &lt; i; j++) &#123;            if (arr[j - 1] &gt; arr[j]) &#123;                flag = false;                int temp = arr[j - 1];                arr[j - 1] = arr[j];                arr[j] = temp;            &#125;        &#125;        if (flag) &#123;            break;        &#125;    &#125;&#125;\n\n","categories":["排序算法"],"tags":["排序算法","冒泡排序"]},{"title":"开发中使用 Redis 查询 KEY 为什么要用 SCAN 而不是 KEYS?(游标 SCAN 的使用)","url":"/2022/01/09/%E5%BC%80%E5%8F%91%E4%B8%AD%E4%BD%BF%E7%94%A8-Redis-%E6%9F%A5%E8%AF%A2-KEY-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8-SCAN-%E8%80%8C%E4%B8%8D%E6%98%AF-KEYS-%E6%B8%B8%E6%A0%87-SCAN-%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"参考博客：\n9. Redis中游标迭代器(scan) - 古明地盆 - 博客园\nRedis SCAN 命令 | 菜鸟教程\n分析一般来说，如果我们需要往 Redis 中查询一些 key，那么首先都会第一时间想到使用 keys 命令。keys 后面接一个模式，即可返回所有匹配指定模式的 key。并且指定模式的时候，可以使用通配符，比如：\n\n* 匹配任意多个任意字符\n? 匹配单个任意字符\n[...] 匹配[]中的任意一个字符\n\n显而易见，keys 使用起来十分的简单、方便。但是其存在着两个比较明显的缺点：\n\n没有分页功能，只能一次性查询出所有符合条件的 key 值，如果查询结果非常巨大，则得到的key 也会非常多\nkeys 命令是遍历查询，等于将数据库中的 key 和指定的模式一一对比，看是否匹配，因此它的查询时间复杂度是 O(n)，所以如果 Redis 中的 key 数量非常的多，则查询时间将会非常的久\n\n因此为了解决这一点，Redis 在2.8版本的时候提出了一个 scan 命令，主要用于解决 keys 命令可能导致整个 Redis 实例停顿的问题。\nSCAN 介绍Redis Scan 命令用于迭代数据库中的数据库键。\nSCAN 命令是一个基于游标的迭代器，每次被调用之后， 都会向用户返回一个新的游标， 用户在下次迭代时需要使用这个新游标作为 SCAN 命令的游标参数， 以此来延续之前的迭代过程。\nSCAN 返回一个包含两个元素的数组， 第一个元素是用于进行下一次迭代的新游标， 而第二个元素则是一个数组， 这个数组中包含了所有被迭代的元素。如果新游标返回 0 表示迭代已结束。\n由于scan命令需要执行多次，即相当于执行了多个命令，存在多次命令请求和响应周期，故整体执行时间可能要比keys命令长。相关命令：\n\nSSCAN 命令用于迭代集合键中的元素。\nHSCAN 命令用于迭代哈希键中的键值对。\nZSCAN 命令用于迭代有序集合中的元素（包括元素成员和元素分值）。\n\nSCAN 使用SCAN cursor [MATCH pattern] [COUNT count]\n\n\ncursor - 游标。\npattern - 匹配的模式。\ncount - 指定从数据集里返回多少元素，默认值为 10 。\n\n不加 pattern 和 count，默认返回 10 个，且所有的 key 都满足条件127.0.0.1:6379&gt; scan 01) &quot;5&quot;2)  1) &quot;all&quot;    2) &quot;a&quot;    3) &quot;ass&quot;    4) &quot;acs&quot;    5) &quot;aii&quot;    6) &quot;abc&quot;    7) &quot;app&quot;    8) &quot;\\xac\\xed\\x00\\x05t\\x00\\x0c201824111111&quot;    9) &quot;aoo&quot;   10) &quot;b&quot;\n\n加上 pattern 和 count# 返回 5 个以 a 开头的 key127.0.0.1:6379&gt; scan 0 match a* count 51) &quot;1&quot;2) 1) &quot;all&quot;   2) &quot;a&quot;   3) &quot;ass&quot;   4) &quot;acs&quot;   5) &quot;aii&quot;   6) &quot;abc&quot;\n\n","categories":["中间件","Redis"],"tags":["Redis"]},{"title":"插入排序","url":"/2022/01/09/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/","content":"实现思路把 n 个待排序的数组看成为一个有序表和一个无序表，开始时有序表中只包含一个元素，无序表中包含有 n-1 个元素，排序过程中每次从无序表中取出第一个元素，依次与有序表中的元素进行比较，将它插入到有序表中适当位置，使之成为新的有序表。\n动态图演示排序过程\n复杂度与稳定性\n平均的时间复杂度为：O( n^2 )\n平均的空间复杂度为：O(1)\n插入排序是一种稳定的排序算法\n\n代码实现public static void insertSort(int[] arr) &#123;    for (int i = 1; i &lt; arr.length; i++) &#123;        int temp = arr[i];        for (int j = i - 1; j &gt;= 0; j--) &#123;            if (temp &lt; arr[j]) &#123;                arr[j + 1] = arr[j];                if (j == 0) &#123;                    arr[j] = temp;                    break;                &#125;            &#125; else &#123;                arr[j + 1] = temp;                break;            &#125;        &#125;    &#125;&#125;\n\n","categories":["排序算法"],"tags":["排序算法","插入排序"]},{"title":"MySQL 索引结构","url":"/2021/04/19/%E7%B0%87%E6%8B%A5%E7%B4%A2%E5%BC%95%E5%92%8C%E9%9D%9E%E7%B0%87%E6%8B%A5%E7%B4%A2%E5%BC%95/","content":"参考文章链接：https://www.cnblogs.com/jiawen010/p/11805241.html\n索引结构MySQL 底层采用 B+ 树结构实现数据索引；MySql 索引数据结构对经典的B+Tree 进行了优化。在原 B+Tree 的基础上，增加一个指向相邻叶子节点的链表指针，就形成了带有顺序指针的 B+Tree，提高区间访问的性能\n如下图所示：\n\n聚簇索引 &amp; 辅助索引（非聚簇索引）描述：聚簇索引：聚簇索引就是按照每张表的主键构造一颗B+树，同时叶子节点中存放的就是整张表的行记录数据，也将聚集索引的叶子节点称为数据页。这个特性决定了索引组织表中数据也是索引的一部分，每张表只能拥有一个聚簇索引\nInnodb通过主键聚集数据，如果没有定义主键，innodb会选择非空的唯一索引代替。如果没有这样的索引，innodb会隐式的定义一个主键来作为聚簇索引\n辅助索引：在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找。辅助索引叶子节点存储的不再是行的物理位置，而是主键值。通过辅助索引首先找到的是主键值，再通过主键值找到数据行的数据页，再通过数据页中的Page Directory找到数据行\nInnodb辅助索引的叶子节点并不包含行记录的全部数据，叶子节点除了包含键值外，还包含了相应行数据的聚簇索引键（主键）\n辅助索引的存在不影响数据在聚簇索引中的组织，所以一张表可以有多个辅助索引。在innodb中有时也称辅助索引为二级索引\n聚簇索引的优缺点：优点：\n数据访问更快，因为聚簇索引将索引和数据保存在同一个B+树中，因此从聚簇索引中获取数据比非聚簇索引更快\n聚簇索引对于主键的排序查找和范围查找速度非常快\n\n缺点：\n插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列为主键\n更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新\n二级索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据\n\n聚簇索引和非聚簇索引的区别\n聚簇索引的叶子节点存放的是主键值和数据行，支持覆盖索引；二级索引的叶子节点存放的是主键值或指向数据行的指针\n由于节子节点(数据页)只能按照一颗B+树排序，故一张表只能有一个聚簇索引。辅助索引的存在不影响聚簇索引中数据的组织，所以一张表可以有多个辅助索引\n\nMyISAM 索引实现MyISAM 索引文件和数据文件是分离的，索引文件仅保存数据记录的地址\n主键索引（非聚簇索引）MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。\n下图是MyISAM主键索引的原理图：\n\n这里设表一共有三列，假设我们以Col1为主键，图myisam1是一个MyISAM表的主索引（Primary key）示意。可以看出MyISAM的索引文件仅仅保存数据记录的地址\n辅助索引在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。\n如果我们在Col2上建立一个辅助索引，则此索引的结构如下图所示：\n\n同样也是一颗B+Tree，data域保存数据记录的地址。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录\nMyISM使用的是非聚簇索引，非聚簇索引的两棵B+树看上去没什么不同，节点的结构完全一致只是存储的内容不同而已，主键索引B+树的节点存储了主键，辅助键索引B+树存储了辅助键。表数据存储在独立的地方，这两颗B+树的叶子节点都使用一个地址指向真正的表数据，对于表数据来说，这两个键没有任何差别。由于索引树是独立的，通过辅助键检索无需访问主键的索引树\nInnoDB 索引实现InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同\n主键索引（聚簇索引）MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引\n\n(图inndb主键索引）是InnoDB主索引（同时也是数据文件）的示意图，可以看到叶节点包含了完整的数据记录。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据记录的列作为主键，如果不存在这种列，则MySQL自动为InnoDB表生成一个隐含字段作为主键，这个字段长度为6个字节，类型为长整形\nInnoDB 的辅助索引InnoDB的所有辅助索引都引用主键作为data域。\n例如，下图为定义在Col3上的一个辅助索引：\n\n聚集索引这种实现方式使得按主键的搜索十分高效，但是辅助索引搜索需要检索两遍索引：首先检索辅助索引获得主键，然后用主键到主索引中检索获得记录\n为什么不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大\n下面部分参考文章链接：https://blog.csdn.net/bible_reader/article/details/100008009\n注意点\n数据文件本身就是索引文件\n表数据文件本身就是按B+Tree组织的一个索引结构文件\n聚集索引中叶节点包含了完整的数据记录\nInnoDB表必须要有主键，并且推荐使用整型自增主键\n\n为什么推荐使用整型自增主键而不是选择UUID？\n\nUUID是字符串，比整型消耗更多的存储空间\n在B+树中进行查找时需要跟经过的节点值比较大小，整型数据的比较运算比字符串更快速\n自增的整型索引在磁盘中会连续存储，在读取一页数据时也是连续；UUID是随机产生的，读取的上下两行数据存储是分散的，不适合执行where id &gt; 5 &amp;&amp; id &lt; 20的条件查询语句\n在插入或删除数据时，整型自增主键会在叶子结点的末尾建立新的叶子节点，不会破坏左侧子树的结构；UUID主键很容易出现这样的情况，B+树为了维持自身的特性，有可能会进行结构的重构，消耗更多的时间\n\n为什么非主键索引结构叶子节点存储的是主键值？\n保证数据一致性和节省存储空间\n\n    在Innodb下主键索引是聚集索引，在Myisam下主键索引是非聚集索引\n\n\nMyISAM 和 InnoDB 的区别myisam 引擎是5.1版本之前的默认引擎，支持全文检索、压缩、空间函数等，但是不支持事务和行级锁，所以一般用于有大量查询少量插入的场景来使用，而且 myisam 不支持外键，并且索引和数据是分开存储的\ninnodb 是基于聚簇索引建立的，和 myisam 相反它支持事务、外键，并且通过 MVCC 来支持高并发，索引和数据存储在一起\n","categories":["数据库","MySQL"],"tags":["数据库","索引结构","MySQL"]},{"title":"给Hexo博客添加看板娘","url":"/2022/01/10/%E7%BB%99Hexo%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0%E7%9C%8B%E6%9D%BF%E5%A8%98/","content":"安装插件npm install --save hexo-helper-live2d\n\n挑选自己喜欢的看板娘安装这里安装的是 live2d-widget-model-hibiki\nnpm install live2d-widget-model-hibiki\n\n看板娘预览https://blog.csdn.net/wang_123_zy/article/details/87181892\n在当前主题的 _config.yml 文件中添加如下配置live2d:  enable: true  scriptFrom: local  model:      use: live2d-widget-model-hibiki #模型选择  display:       position: right  #模型位置      width: 100       #模型宽度      height: 200      #模型高度  mobile:       show: false      #是否在手机端显示\n\n","categories":["Hexo"],"tags":["Hexo"]},{"title":"选择排序","url":"/2022/01/09/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/","content":"实现思路\n定义一个长度为 n 的无序数组。\n第一次遍历整个数组，找到最小的数，然后和数组的第一个数交换。\n第二次从数组第二个数开始往下遍历，找到最小的数，然后和数组的第二个数交换。\n以此类推，经过 n - 1 轮遍历交换后，数组整体有序。\n排序结束。\n\n动态图演示排序过程\n复杂度与稳定性\n平均时间复杂度为：O( n^2 )\n平均空间复杂度为：O(1)\n选择排序是一种不稳定的排序算法\n\n代码实现public static void selectSort(int[] arr) &#123;    for (int i = 0; i &lt; arr.length - 1; i++) &#123;        int index = i;        for (int j = i + 1; j &lt; arr.length; j++) &#123;            if (arr[j] &lt; arr[index ]) &#123;                index = j;            &#125;        &#125;        if (index != i) &#123;            int temp = arr[i];            arr[i] = arr[index];            arr[index] = temp;        &#125;    &#125;&#125;\n\n","categories":["排序算法"],"tags":["排序算法","选择排序"]},{"title":"Redis 五种数据结构底层实现","url":"/2021/04/19/Redis%20%E4%BA%94%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/","content":"Redis参考文章链接：\n整体架构： https://blog.csdn.net/weixin_45017459/article/details/115802399?spm=1001.2014.3001.5501\nString 部分：https://blog.csdn.net/qq_41446768/article/details/98969850\nZset 部分：https://blog.csdn.net/weichi7549/article/details/107335133/\n淘汰策略部分：https://zhuanlan.zhihu.com/p/105587132\n一、概述Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射\n键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合\nRedis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能\n二、数据类型\n\n\n数据类型\n可以存储的值\n\n\n\nSTRING\n字符串、整数或者浮点数\n\n\nLIST\n列表\n\n\nSET\n无序集合\n\n\nHASH\n包含键值对的无序散列表\n\n\nZSET\n有序集合\n\n\nStringRedis的字符串是动态字符串，是可以修改的字符串，内部实现类似于Java的 ArrayList，实际上是一种名为简单动态字符串(SDS Simple Dynamic String)的抽象类型，在redis的数据库里面包含字符串值的键值对都是SDS实现的\nstruct sdshdr &#123;       // 记录buf数组中已使用字节的数量      // 等于SDS所保存字符串的长度      int len;       // 记录buf数组中未使用字节的数量      int free;       // 字节数组，用于保存字符串      char buf[];   &#125;;\n\n一个sds的存储示例：\n\nFree等于0代表当前已满，没有任何未分配的空间\nlen等于5代表存储了五字节长的字符串\nbuf属性是一个char类型的数组,数组的前五个字节分别保存了r、e、d、i、s五个字符串，而最后一个字节则保存了’\\0’\nSDS遵循C字符串以空字符串结尾的惯例，保存空字符串的一字节空间不计算到SDS的len属性里面，并且为空字符串分配额外的一字节空间，以及添加空字符串到字符串末尾等操作，都是由SDS函数自动完成的,所以这个空字符串对于SDS的使用者来说是完全透明的\n\nSDS的扩容机制：\n当字符串小于1M时，扩容都是加倍现有的空间，实际的容量就是(当前容量*2+1)额外的一字节用于保存空字符串\n如果超过1M时,扩容时一次只会多扩1M的空间，实际的容量是（当前容量+1M+1byte）。需要注意的是字符串最大长度为512M\n\n使用 SDS 而不是用字符串的原因：\n降低获取字符串长度的时间复杂度到O(1)\n减少了修改字符串时的内存重分配次数\n兼容c字符串的同时，提高了一些字符串工具方法的效率\n二进制安全（数据写入的格式和读取的格式一致）\n\nList（列表）底层实现：\n底层用 quicklist 双向链表存储，而 quicklist 的每一个结点包含着一个 ziplist；总体来说 list 底层使用 quicklist + 双向链表实现的\n\nziplist\n介绍\n一个特殊的双向链表；没有维护一对指向前后结点的指针（prev、next），而是存储上一个 entry 的长度和当前 entry的长度，通过长度推算下一个元素的位置\n典型的“时间换空间”：牺牲读取的性能，获取高效的存储空间；因为存储指针比存储 entry 长度更费内存（字符串简短的情况）\n字段、值比较小才会用 ziplist\n\n\n优缺点\nziplist 存储在一段连续的内存上，存储效率很高\n不利于增删改操作，因为需要频繁的申请和释放内存\n\n\n\nquicklist每个节点都是以压缩列表ziplist的结构保存着数据，而每个ziplist又可以包含多个entry。也可以说一个quicklist节点保存的是一片数据，而不是一个数据，即每个quicklist节点就是一个ziplist，具备压缩列表的特性。整体上quicklist就是一个双向链表结构，和普通的链表操作一样，插入删除效率很高，但查询的效率却是O(n)。不过，这样的链表访问两端的元素的时间复杂度却是O(1)。所以，对list的操作多数都是poll和push\nSet特点\n存储大量数据、查询速度块\n集合中的数据是不重复且没有顺序\n集合类型的Redis内部是使用值为空的散列表实现，所有这些操作的时间复杂度都为0(1)\n集合类型的常用操作是向集合中加入或删除元素、判断某个元素是否存在等，除此之外Redis还提供了多个集合之间的交集、并集、差集的运算\n\nhash 和 set 的结构\nhash: key-{field:value}\nset: key-{value:null}\n\nHashhash叫散列类型，它提供了字段和字段值的映射。字段值只能是字符串类型，不支持其它类型\n\n格式：一个存储空间(key)存储多个键值对，底层通过哈希表进行存储\n如果filed数量较少时，会被优化为类数组（ziplist）的结构，如果filed数量多，就是HashMap\nhash最初设计不是为了存对象，不要把hash当成对象列表使用\n扩展和收缩哈希表的工作可以通过执行rehash（重新散列）操作来完成，步骤如下：\n\n为字典的ht[1]哈希表分配空间，空间大小根据实际情况而定\n\n将ht[0]中所有键值对rehash到ht[1]中\n\n注意：rehash指的是重新计算键的哈希值和索引值，然后将键值对放置到ht[1]哈希表的指定位置上\n\n\n释放ht[0]，将ht[1]设置为ht[0]，并在ht[1]新建一个空表，为下次rehash做准备\n\n\n应用场景电商购物车：添加购物车、浏览购物车商品、更改购物车商品数量、删除商品、清空商品均可实现。  key : userID  field : 商品ID  value : 商品购买数量\nZset（跳跃表）Zset 编码的选择\n有序集合对象的编码可以是ziplist或者skiplist。同时满足以下条件时使用ziplist编码：\n\n元素数量小于128个\n所有member的长度都小于64字节\n\nskiplist1. 介绍skiplist 编码的 Zset 底层为一个被称为 zset 的结构体，这个结构体中包含一个字典和一个跳跃表。跳跃表按 score 从小到大保存所有集合元素，查找时间复杂度为平均 O(logN)，最坏 O(N) 。字典则保存着从 member 到 score 的映射，这样就可以用 O(1)的复杂度来查找 member 对应的 score 值。虽然同时使用两种结构，但它们会通过指针来共享相同元素的 member 和 score，因此不会浪费额外的内存\n2. 详解跳表(skip List)是一种随机化的数据结构，基于并联的链表，实现简单，插入、删除、查找的复杂度均为O(logN)。简单说来跳表也是链表的一种，只不过它在链表的基础上增加了跳跃功能，正是这个跳跃的功能，使得在查找元素时，跳表能够提供O(logN)的时间复杂度\n先来看一个有序链表，如下图（最左侧的灰色节点表示一个空的头结点）：\n\n在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置\nskiplist 不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程：\n\n从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案\nskiplist，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度\n假设我们在它里面依然查找23，下图给出了查找路径：\n\n需要注意的是，前面演示的各个节点的插入过程，实际上在插入之前也要先经历一个类似的查找过程，在确定插入位置后，再完成插入操作\n实际应用中的skiplist每个节点应该包含key和value两部分。前面的描述中我们没有具体区分key和value，但实际上列表中是按照key(score)进行排序的，查找过程也是根据key在比较\n三、使用场景\n缓存\n将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率\n\n\n消息队列\nList 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息\n不过最好使用 Kafka、RabbitMQ 等消息中间件\n\n\n会话缓存\n可以使用 Redis 来统一存储多台应用服务器的会话信息\n当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性\n\n\n分布式锁实现\n在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步\n可以使用 Redis 自带的 SETNX 命令实现分布式锁，他跟set相反，set会把值覆盖，而SETNX不会覆盖，会除此之外，还可以使用官方提供的 RedLock 分布式锁实现\n\n\n\n四、淘汰策略1. 前置知识\n过期策略\n定期删除：redis默认是每隔 100ms就随机抽取一些设置了过期时间的key，检查其是否过期，如果过期就删除\n惰性删除：在客户端访问这个key的时候，redis对key的过期时间进行检查，如果过期了就立即删除，不会给你返回任何东西\n\n\n\n总结：定期删除是集中处理，惰性删除是零散处理\n2. 为什么需要淘汰策略因为不管是定期采样删除还是惰性删除都不是一种完全精准的删除，就还是会存在key没有被删除掉的场景，所以就需要内存淘汰策略进行补充\n3. 内存淘汰策略\nnoeviction：当内存使用超过配置的时候会返回错误，不会驱逐任何键\nallkeys-lru：加入键的时候，如果过限，首先通过LRU算法驱逐最久没有使用的键\nvolatile-lru：加入键的时候如果过限，首先从设置了过期时间的键集合中驱逐最久没有使用的键\nallkeys-random：加入键的时候如果过限，从所有key随机删除\nvolatile-random：加入键的时候如果过限，从过期键的集合中随机驱逐\nvolatile-ttl：从配置了过期时间的键中驱逐马上就要过期的键\nvolatile-lfu：从所有配置了过期时间的键中驱逐使用频率最少的键\nallkeys-lfu：从所有键中驱逐使用频率最少的键\n\n五、持久化Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上\nRDB（快照）持久化原理：RDB文件存在是以一个压缩后的二进制文件。在某些时刻，redis 通过 fork（RDB会通过一个 bgsave 的命令，会 fork 出一个子进程） 产生一条子进程以及一个父进程的快照（副本），快照中有和父进程当前时刻相同的数据；此时父进程继续处理 client 请求执行 IO 操作；子进程负责将快照写入临时文件，子进程写完后，用临时文件替换原来的快照文件，然后子进程销毁；此操作每隔一定的时间就会执行一次。\nAOF（日志）持久化原理：将写命令添加到 AOF 文件（Append Only File）的末尾\n将“操作 + 数据”以格式化指令的方式追加到缓冲区中，然后缓冲区根据对应的策略向硬盘进行同步操作\nAOF保存了历史所有的操作过程。当server需要数据恢复时，可以直接加载日志文件，即可还原所有的操作过程\n使用 AOF 持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项：Redis目前支持三种AOF保存模式：\n\n    everysec：表示每秒同步一次(默认值，很快，但可能会丢失一秒以内的数据\n    no：表示等操作系统进行数据缓存同步到磁盘，效率快，但是持久化没保证\n    always：同步持久化，每次发生数据变更时，立即记录到磁盘,效率慢，严重减低服务器的性能，但是安全\n\n\n六、实现高可用1. 主从复制为了分担压力，Redis支持主从复制，Redis的主从结构可以采用一主多从或者级联结构，Redis主从同步策略的策略就是先是全量同步，再为增量同步\n全量同步：Redis全量复制一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份。具体步骤如下：\n\n\n从服务器连接主服务器\n主服务器接收到命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令\n主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令\n从服务器收到快照文件后丢弃所有旧数据，载入收到的快照\n主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令\n从服务器完成对快照的载入，开始接收命令请求，此后主服务器每执行一次写命令，就向从服务器发送相同的写命令（也就是增量同步）\n\n\n增量同步：Redis增量复制是指Slave初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。 增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令\n主从复制的作用：主从复制，读写分离，容灾恢复。一台主机负责写入数据，多台从机负责备份数据。在高并发的场景下，即便是主机挂了，可以用从机代替主机继续工作，避免单点故障导致系统性能问题。读写分离，让读多写少的应用性能更佳\n2. 哨兵使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主从服务器出现故障宕机时，需要手动进行恢复，为了解决这个问题，Redis 增加了哨兵模式（因为哨兵模式做到了可以监控主从服务器，并且提供自动容灾恢复的功能）。 Sentinel（哨兵）可以监听集群中的服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器\n\n3. 集群七、更新缓存机制1. 先更新数据库，在删除缓存（推荐）若数据库更新成功，删除缓存操作失败，则此后读到的都是缓存中过期的数据，造成不一致问题\n可以设置过期时间\n八、缓存雪崩/击穿/穿透1. 缓存穿透\n描述\n\n指查询一个一定不存在的数据，由于缓存是不命中，将去查询数据库，但是数据库也无此记录，我们没有将这次查询的null写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。\n\n风险\n\n利用不存在的数据及进行攻击，数据库瞬时压力增大，最终导致崩溃。\n\n解决方案1\n\nnull结果也放到缓存中，并加入短暂过期时间。\n\n解决方案2\n\n\n布隆过滤器\n布隆过滤器是一种数据结构，对所有可能查询的参数以hash形式存储，在控制层先进性校验，不符合则丢弃，从而避免了对底层存储系统的查询压力。\n\n2. 缓存雪崩\n描述\n\n缓存雪崩是指我们设置缓存时key采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重导致雪崩。\n\n解决方案\n\n原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。\n3. 缓存击穿\n描述\n\n\n对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。\n如果这个key在大量请求同时进来前正好失效，那么所有对这个key的数据查询都落到DB，我们成为缓存击穿\n\n\n解决方案\n\n\n加锁\n大量并发只让一个人去查，其他人等待，查到以后释放锁，其他人获取到锁，先查缓存，就会有数据，不用去DB\n\n","categories":["中间件","Redis"],"tags":["中间件","Redis","缓存"]}]